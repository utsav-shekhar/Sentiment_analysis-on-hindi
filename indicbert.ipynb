{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7841053,"sourceType":"datasetVersion","datasetId":4596788},{"sourceId":7843725,"sourceType":"datasetVersion","datasetId":4598797}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm import tqdm\n\n# Step 1: Load your dataset\nparquet_file_path1 = \"/kaggle/input/sentiment-dataset/train-00000-of-00001 (1).parquet\"\nparquet_file_path2 = \"/kaggle/input/sentiment-dataset/train-00000-of-00001.parquet\"\n\ndf1 = pd.read_parquet(parquet_file_path1)\ndf2 = pd.read_parquet(parquet_file_path2)\ndf = pd.concat([df1, df2], ignore_index=True)\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# Step 2: Tokenization\ntokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n\n# Step 3: Create Dataset and DataLoader\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        text = self.dataframe.iloc[idx]['text']\n        label = self.dataframe.iloc[idx]['label']\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Define constants\nMAX_LENGTH = 128  # adjust as needed\nBATCH_SIZE = 32  # adjust as needed\n\n# Create dataset and dataloader\ndataset = CustomDataset(df, tokenizer, max_length=MAX_LENGTH)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Step 4: Define the Model\nclass MyClassifier(torch.nn.Module):\n    def __init__(self, num_classes):\n        super(MyClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained(\"ai4bharat/indic-bert\")\n        self.dropout = torch.nn.Dropout(0.1)\n        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return logits\n\n# Step 5: Training Loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MyClassifier(num_classes=3)  # assuming 3 classes\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\ncriterion = torch.nn.CrossEntropyLoss()\n\nmodel.train()\nfor epoch in range(5):\n    tqdm_dataloader = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n    for batch in tqdm_dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        tqdm_dataloader.set_postfix({'loss': loss.item()})\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T16:53:11.627754Z","iopub.execute_input":"2024-03-15T16:53:11.628112Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92caca2075ba4aae8198436a6c0b90fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6d998d7ec13498c82c3cf5b563e9af6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28ae0386cf8d450da96193f244ff4b42"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nEpoch 1:  61%|██████    | 128/209 [40:19<25:18, 18.75s/it, loss=1.09] ","output_type":"stream"}]},{"cell_type":"code","source":"# Step 6: Evaluation (using a separate test dataset)\n# Similar to the training loop, but set the model to evaluation mode and don't perform backpropagation\n\n# Step 1: Load your test dataset\ntest_parquet_file_path = \"/kaggle/input/testing/test-00000-of-00001.parquet\"\ntest_df = pd.read_parquet(test_parquet_file_path)\n\n# Step 2: Create DataLoader for testing\ntest_dataset = CustomDataset(test_df, tokenizer, max_length=MAX_LENGTH)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Step 3: Evaluation Loop\nmodel.eval()\ntest_accuracy = 0.1\ntotal_test_samples = 0\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, predicted_labels = torch.max(logits, dim=1)\n        \n        test_accuracy += (predicted_labels == labels).sum().item()\n        total_test_samples += labels.size(0)\n\ntest_accuracy /= total_test_samples\nprint(f\"Test Accuracy: {test_accuracy}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}